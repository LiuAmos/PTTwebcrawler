{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\amos\\Desktop\\PTTwebcrawler\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\amos\\AppData\\Local\\Temp\\jieba.u1ad717f44f295317eaab682294acefca.cache\n",
      "Loading model cost 1.382 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "#jieba.set_dictionary('dict.txt.big')\n",
    "jieba.set_dictionary('dict.txt.big.txt')\n",
    "\n",
    "j=1\n",
    "path='C:\\\\Users\\\\amos\\\\Desktop\\\\PTTwebcrawler\\\\food\\\\'\n",
    "#path='C:\\\\Users\\\\User\\\\Desktop\\\\PTTwebcrawler\\\\food\\\\'\n",
    "list_food=[]\n",
    "\n",
    "#print(list_food)\n",
    "for i in range (1,101):\n",
    "    f = open(path+'f_'+str(j)+'.text','r',encoding = 'utf8')\n",
    "    #content=f.readlines()\n",
    "    content=f.read()\n",
    "    eachdoc_cut=jieba.cut(content,cut_all=False)\n",
    "    eachdoc_str=''\n",
    "    for k in eachdoc_cut:\n",
    "        #print(k)\n",
    "        eachdoc_str=eachdoc_str+k\n",
    "        eachdoc_str=eachdoc_str+' '\n",
    "    #eachdoc_str=eachdoc_str.strip()\n",
    "    list_food.append(eachdoc_str)\n",
    "\n",
    "    f.close()\n",
    "    j=j+1\n",
    "# for q in range (0,2):\n",
    "#     print(list_food[q])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(list_food))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10939\n",
      "10939\n"
     ]
    }
   ],
   "source": [
    "len_list_food=len(list_food)\n",
    "tfidf_matrix_toarray=[]\n",
    "tfidf_matrix_array=[]\n",
    "list_feature=[]\n",
    "feature_counter=0\n",
    "#analyzer=lambda x:x.split(' ')\n",
    "tfidfVecorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidfVecorizer.fit_transform(list_food)\n",
    "wordOfFood = tfidfVecorizer.get_feature_names()\n",
    "tfidf_matrix_toarray=tfidf_matrix.toarray()\n",
    "# print(tfidf_matrix[78,0])\n",
    "# print('---------')\n",
    "print(len(wordOfFood))\n",
    "# c=0\n",
    "# for a in range(0,len(tfidf_matrix.toarray())):\n",
    "#     for b in range(0,len(tfidf_matrix.toarray()[0])):\n",
    "#         tfidf_matrix_array.append(tfidf_matrix_toarray[a][b])\n",
    "#         c=c+1\n",
    "# tfidf_matrix_array.sort(reverse = True)\n",
    "#print(tfidf_matrix[0,1])\n",
    "#print(type(tfidf_matrix))\n",
    "\n",
    "# for feature_number in range(0,5):\n",
    "#     print(tfidf_matrix_array[feature_number])\n",
    "\n",
    "\n",
    "term2id_dict = tfidfVecorizer.vocabulary_\n",
    "new_dict = {v : k for k, v in term2id_dict.items()}\n",
    "\n",
    "print(len(new_dict))\n",
    "#print(tfidf_matrix[10,12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "[4116, 9343, 7412, 7199, 10647, 10788, 6386, 6867, 9376, 1039, 1376, 10736, 7393, 7072, 7345, 1085, 1094, 10452, 1386, 6769, 1064, 3725, 2252, 10344, 3792, 4474, 10134, 10781, 1007, 1316, 939, 1367, 4656, 10243, 55, 870, 1153, 1277, 1372, 2719, 5397, 6426, 1445, 7152, 8715, 9448, 5123, 6671, 23, 5980, 8989, 8994, 8930, 1144, 1191, 1145, 10666, 1875, 10746, 3097]\n",
      "['壽司', '豆花', '牡蠣', '烏龍', '香腸', '鮭魚', '柚子', '海南', '貓鼠', 'ethanmps', 'photos', '鬆餅', '牛排', '滿意度', '燒鵝', 'gl', 'goo', '韓牛', 'pizza', '河粉', 'flickr', '咖哩', '乳酪', '雞肉飯', '商號', '好豆堂', '銷魂', '魯肉', 'due', 'nt', 'circle', 'pasta', '寶石', '阿蘭', '0rz', 'bar', 'italian', 'merveille', 'petite', '優果', '慢磨', '梁記', 'rou', '火鍋', '芒果', '赤鬼', '彰化', '水伯', '06', '日常生活', '蛋糕', '蛋餅', '薯條', 'imgur', 'jpg', 'imksw', '馬場', '三峽', '魚刺', '刨冰']\n"
     ]
    }
   ],
   "source": [
    "# print(np.max(tfidf_matrix_toarray))\n",
    "# maxx = np.where(tfidf_matrix_toarray==np.max(tfidf_matrix_toarray))\n",
    "# print(maxx[1][0])\n",
    "Max_60words_food=[]\n",
    "for i in range(0,60):\n",
    "    maxx = np.where(tfidf_matrix_toarray==np.max(tfidf_matrix_toarray))\n",
    "    #print(maxx)\n",
    "    #print(wordOfMobile[maxx[1][0]] , tfidf[maxx[0][0],maxx[1][0]])\n",
    "    Max_60words_food.append(maxx[1][0])\n",
    "    for j in range(0,100):\n",
    "        tfidf_matrix_toarray[j,maxx[1][0]] = 0\n",
    "    i = i+1\n",
    "print(len(Max_60words_food)) \n",
    "print(Max_60words_food) \n",
    "Food_60 = []\n",
    "for i in Max_60words_food :\n",
    "    Food_60.append(wordOfFood[i])\n",
    "print(Food_60)\n",
    "# for i in Max_60words_food:\n",
    "#     print(wordOfFood[i])\n",
    "# for i in range(0,60):\n",
    "#     maxx = np.where(tfidf_np_Mobile==np.max(tfidf_np_Mobile))\n",
    "# #     print(wordOfMobile[maxx[1][0]] , tfidf[maxx[0][0],maxx[1][0]])\n",
    "#     Max_60words_Mobile.append(maxx[1][0])\n",
    "#     for j in range(0,100):\n",
    "#         tfidf_np_Mobile[j,maxx[1][0]] = 0\n",
    "\n",
    "#     i = i+1\n",
    "# for ft in range(0,3):\n",
    "#     break_flag=False\n",
    "#     for zi in range(0,100):\n",
    "#         #print(z)\n",
    "#         for wi in range (0,len(new_dict)):\n",
    "\n",
    "#             if tfidf_matrix_array[feature_counter]==tfidf_matrix[zi,wi]:\n",
    "#                 print(zi,wi)\n",
    "#                 print(new_dict[wi])\n",
    "# #                 print(z,w)\n",
    "# #                 print(new_dict[w])\n",
    "#                 if new_dict[wi] not in list_feature:\n",
    "        \n",
    "#                     list_feature.append(new_dict[wi])\n",
    "#                     feature_counter=feature_counter+1\n",
    "#                     #print(feature_counter)\n",
    "#                     break_flag=True\n",
    "#                     break\n",
    "#         if break_flag:\n",
    "#             break\n",
    "#print(type(term2id_dict))\n",
    "#print(term2id_dict)\n",
    "\n",
    "#print('\\n'.join(tags))\n",
    "#print(list_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "壽司,豆花,牡蠣,烏龍,香腸,鮭魚,柚子,海南,貓鼠,ethanmps,photos,鬆餅,牛排,滿意度,燒鵝,gl,goo,韓牛,pizza,河粉,flickr,咖哩,乳酪,雞肉飯,商號,好豆堂,銷魂,魯肉,due,nt,circle,pasta,寶石,阿蘭,0rz,bar,italian,merveille,petite,優果,慢磨,梁記,rou,火鍋,芒果,赤鬼,彰化,水伯,06,日常生活,蛋糕,蛋餅,薯條,imgur,jpg,imksw,馬場,三峽,魚刺,刨冰\n"
     ]
    }
   ],
   "source": [
    "tags=''\n",
    "# list_tags=[]\n",
    "# for sixty in range(0,60):\n",
    "#     print(new_dict[Max_60words_food[sixty]])\n",
    "#     list_tags.append(new_dict[Max_60words_food[sixty]])\n",
    "tags=\",\".join(str(i) for i in Food_60)\n",
    "print(tags)\n",
    "\n",
    "f = open(path+'f_0'+'.text','w',encoding = 'utf8')\n",
    "f.write(tags)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(list_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
