{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\amos\\Desktop\\PTTwebcrawler\\dict.txt.big.txt ...\n",
      "Dumping model to file cache C:\\Users\\amos\\AppData\\Local\\Temp\\jieba.u1ad717f44f295317eaab682294acefca.cache\n",
      "Loading model cost 1.606 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "#jieba.set_dictionary('dict.txt.big')\n",
    "jieba.set_dictionary('dict.txt.big.txt')\n",
    "\n",
    "j=1\n",
    "path='C:\\\\Users\\\\amos\\\\Desktop\\\\PTTwebcrawler\\\\food\\\\'\n",
    "list_food=[]\n",
    "eachdoc_str=''\n",
    "#print(list_food)\n",
    "for i in range (1,101):\n",
    "    f = open(path+'f_'+str(j)+'.text','r',encoding = 'utf8')\n",
    "    #content=f.readlines()\n",
    "    content=f.read()\n",
    "    eachdoc_cut=jieba.cut(content,cut_all=False)\n",
    "    \n",
    "    for k in eachdoc_cut:\n",
    "        #print(k)\n",
    "        eachdoc_str=eachdoc_str+k+' '\n",
    "    \n",
    "    eachdoc_str=eachdoc_str.strip()\n",
    "    list_food.append(eachdoc_str)\n",
    "\n",
    "    f.close()\n",
    "    j=j+1\n",
    "# for q in range (0,2):\n",
    "#     print(list_food[q])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(list_food))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.177583974707611\n",
      "---------\n",
      "0.4353149047282253\n",
      "0.4254744107891895\n",
      "0.41391589232430015\n",
      "0.40014694569181153\n",
      "0.3860627811166679\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "len_list_food=len(list_food)\n",
    "tfidf_matrix_toarray=[]\n",
    "tfidf_matrix_array=[]\n",
    "list_feature=[]\n",
    "feature_counter=0\n",
    "#analyzer=lambda x:x.split(' ')\n",
    "tfidfVecorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidfVecorizer.fit_transform(list_food)\n",
    "tfidf_matrix_toarray=tfidf_matrix.toarray()\n",
    "print(tfidf_matrix[78,0])\n",
    "print('---------')\n",
    "\n",
    "c=0\n",
    "for a in range(0,len(tfidf_matrix.toarray())):\n",
    "    for b in range(0,len(tfidf_matrix.toarray()[0])):\n",
    "        tfidf_matrix_array.append(tfidf_matrix_toarray[a][b])\n",
    "        c=c+1\n",
    "tfidf_matrix_array.sort(reverse = True)\n",
    "#print(tfidf_matrix[0,1])\n",
    "#print(type(tfidf_matrix))\n",
    "\n",
    "for feature_number in range(0,5):\n",
    "    print(tfidf_matrix_array[feature_number])\n",
    "\n",
    "\n",
    "term2id_dict = tfidfVecorizer.vocabulary_\n",
    "new_dict = {v : k for k, v in term2id_dict.items()}\n",
    "\n",
    "print(len_list_food)\n",
    "#print(tfidf_matrix[10,12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "[9384, 4157, 9505, 5428, 37, 28, 0, 2155, 10688, 3754, 3562, 3519, 7834, 6102, 1179, 324, 4059, 10435, 10829, 7393, 7029, 8130, 962, 1452, 5773, 3518, 10652, 6782, 7629, 1108, 2148, 5504, 1646, 1178, 7434, 5060, 6686, 8474, 990, 7883, 4474, 4963, 10640, 492, 79, 9227, 7236, 1711, 10058, 9, 4836, 23, 5490, 8931, 9963, 3981, 958, 1202, 1600, 3855]\n"
     ]
    }
   ],
   "source": [
    "# print(np.max(tfidf_matrix_toarray))\n",
    "# maxx = np.where(tfidf_matrix_toarray==np.max(tfidf_matrix_toarray))\n",
    "# print(maxx[1][0])\n",
    "Max_60words_food=[]\n",
    "for i in range(0,60):\n",
    "    maxx = np.where(tfidf_matrix_toarray==np.max(tfidf_matrix_toarray))\n",
    "#     print(wordOfMobile[maxx[1][0]] , tfidf[maxx[0][0],maxx[1][0]])\n",
    "    Max_60words_food.append(maxx[1][0])\n",
    "    for j in range(0,100):\n",
    "        tfidf_matrix_toarray[j,maxx[1][0]] = 0\n",
    "    i = i+1\n",
    "print(len(Max_60words_food)) \n",
    "print(Max_60words_food) \n",
    "\n",
    "\n",
    "# for i in range(0,60):\n",
    "#     maxx = np.where(tfidf_np_Mobile==np.max(tfidf_np_Mobile))\n",
    "# #     print(wordOfMobile[maxx[1][0]] , tfidf[maxx[0][0],maxx[1][0]])\n",
    "#     Max_60words_Mobile.append(maxx[1][0])\n",
    "#     for j in range(0,100):\n",
    "#         tfidf_np_Mobile[j,maxx[1][0]] = 0\n",
    "\n",
    "#     i = i+1\n",
    "# for ft in range(0,3):\n",
    "#     break_flag=False\n",
    "#     for zi in range(0,100):\n",
    "#         #print(z)\n",
    "#         for wi in range (0,len(new_dict)):\n",
    "\n",
    "#             if tfidf_matrix_array[feature_counter]==tfidf_matrix[zi,wi]:\n",
    "#                 print(zi,wi)\n",
    "#                 print(new_dict[wi])\n",
    "# #                 print(z,w)\n",
    "# #                 print(new_dict[w])\n",
    "#                 if new_dict[wi] not in list_feature:\n",
    "        \n",
    "#                     list_feature.append(new_dict[wi])\n",
    "#                     feature_counter=feature_counter+1\n",
    "#                     #print(feature_counter)\n",
    "#                     break_flag=True\n",
    "#                     break\n",
    "#         if break_flag:\n",
    "#             break\n",
    "#print(type(term2id_dict))\n",
    "#print(term2id_dict)\n",
    "# f = open(path+'f_0'+'.text','w',encoding = 'utf8')\n",
    "# f.write(tags)\n",
    "# f.close()\n",
    "#print('\\n'.join(tags))\n",
    "#print(list_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "豆花\n",
      "壽司\n",
      "起來\n",
      "感覺\n",
      "08\n",
      "06\n",
      "00\n",
      "不錯\n",
      "香腸\n",
      "味道\n",
      "可以\n",
      "口感\n",
      "相當\n",
      "時間\n",
      "https\n",
      "2018\n",
      "地址\n",
      "電話\n",
      "鮭魚\n",
      "營業時間\n",
      "港式\n",
      "算是\n",
      "cc\n",
      "ptt\n",
      "搭配\n",
      "口味\n",
      "餐點\n",
      "沒有\n",
      "用餐\n",
      "food\n",
      "不過\n",
      "所以\n",
      "www\n",
      "http\n",
      "牛排\n",
      "店名\n",
      "比較\n",
      "義大利\n",
      "com\n",
      "真的\n",
      "好吃\n",
      "帶點\n",
      "餐廳\n",
      "30\n",
      "11\n",
      "覺得\n",
      "烏來\n",
      "一個\n",
      "配料\n",
      "02\n",
      "就是\n",
      "05\n",
      "或是\n",
      "蒸籠\n",
      "還是\n",
      "因為\n",
      "cafe\n",
      "jana\n",
      "tw\n",
      "喜歡\n"
     ]
    }
   ],
   "source": [
    "for sixty in range(0,60):\n",
    "    print(new_dict[Max_60words_food[sixty]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(list_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
